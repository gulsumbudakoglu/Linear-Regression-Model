{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics \n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'your_path\\dataset1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x32</th>\n",
       "      <th>x33</th>\n",
       "      <th>x34</th>\n",
       "      <th>x35</th>\n",
       "      <th>x36</th>\n",
       "      <th>x37</th>\n",
       "      <th>x38</th>\n",
       "      <th>x39</th>\n",
       "      <th>x40</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.768548</td>\n",
       "      <td>-0.812924</td>\n",
       "      <td>0.487731</td>\n",
       "      <td>0.676055</td>\n",
       "      <td>0.660635</td>\n",
       "      <td>-0.444268</td>\n",
       "      <td>-0.893640</td>\n",
       "      <td>0.156825</td>\n",
       "      <td>-0.876694</td>\n",
       "      <td>0.620652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438694</td>\n",
       "      <td>-0.019702</td>\n",
       "      <td>-0.600950</td>\n",
       "      <td>-0.877849</td>\n",
       "      <td>0.569724</td>\n",
       "      <td>-0.932564</td>\n",
       "      <td>0.290253</td>\n",
       "      <td>-0.763400</td>\n",
       "      <td>0.935585</td>\n",
       "      <td>-63.087329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.670920</td>\n",
       "      <td>-0.849962</td>\n",
       "      <td>-0.248050</td>\n",
       "      <td>0.960456</td>\n",
       "      <td>0.157492</td>\n",
       "      <td>0.792894</td>\n",
       "      <td>-0.586935</td>\n",
       "      <td>0.502329</td>\n",
       "      <td>0.981269</td>\n",
       "      <td>-0.620777</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281196</td>\n",
       "      <td>-0.885851</td>\n",
       "      <td>0.416578</td>\n",
       "      <td>-0.820090</td>\n",
       "      <td>0.539046</td>\n",
       "      <td>-0.602434</td>\n",
       "      <td>-0.507292</td>\n",
       "      <td>-0.717584</td>\n",
       "      <td>0.678476</td>\n",
       "      <td>-61.257623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.031980</td>\n",
       "      <td>-0.803569</td>\n",
       "      <td>0.617597</td>\n",
       "      <td>0.846003</td>\n",
       "      <td>0.567892</td>\n",
       "      <td>0.360899</td>\n",
       "      <td>-0.531613</td>\n",
       "      <td>-0.677216</td>\n",
       "      <td>0.611557</td>\n",
       "      <td>0.917289</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104604</td>\n",
       "      <td>0.267912</td>\n",
       "      <td>-0.165641</td>\n",
       "      <td>0.509045</td>\n",
       "      <td>0.703101</td>\n",
       "      <td>0.427531</td>\n",
       "      <td>0.142499</td>\n",
       "      <td>-0.754593</td>\n",
       "      <td>-0.924559</td>\n",
       "      <td>-58.916994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.157307</td>\n",
       "      <td>-0.764734</td>\n",
       "      <td>-0.823678</td>\n",
       "      <td>0.555114</td>\n",
       "      <td>-0.400031</td>\n",
       "      <td>-0.074208</td>\n",
       "      <td>-0.832775</td>\n",
       "      <td>0.881861</td>\n",
       "      <td>0.521265</td>\n",
       "      <td>0.413126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085894</td>\n",
       "      <td>-0.455087</td>\n",
       "      <td>0.662139</td>\n",
       "      <td>-0.846376</td>\n",
       "      <td>0.979447</td>\n",
       "      <td>-0.876542</td>\n",
       "      <td>-0.576039</td>\n",
       "      <td>0.242609</td>\n",
       "      <td>-0.890070</td>\n",
       "      <td>-56.749217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.476609</td>\n",
       "      <td>-0.707830</td>\n",
       "      <td>-0.474722</td>\n",
       "      <td>0.763908</td>\n",
       "      <td>-0.869485</td>\n",
       "      <td>-0.349489</td>\n",
       "      <td>-0.293731</td>\n",
       "      <td>0.929615</td>\n",
       "      <td>0.450917</td>\n",
       "      <td>-0.781646</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014800</td>\n",
       "      <td>0.404468</td>\n",
       "      <td>-0.225323</td>\n",
       "      <td>0.733259</td>\n",
       "      <td>0.470233</td>\n",
       "      <td>0.830019</td>\n",
       "      <td>0.199850</td>\n",
       "      <td>-0.435511</td>\n",
       "      <td>-0.277896</td>\n",
       "      <td>-56.247766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2        x3        x4        x5        x6        x7  \\\n",
       "0  0.768548 -0.812924  0.487731  0.676055  0.660635 -0.444268 -0.893640   \n",
       "1 -0.670920 -0.849962 -0.248050  0.960456  0.157492  0.792894 -0.586935   \n",
       "2  0.031980 -0.803569  0.617597  0.846003  0.567892  0.360899 -0.531613   \n",
       "3  0.157307 -0.764734 -0.823678  0.555114 -0.400031 -0.074208 -0.832775   \n",
       "4 -0.476609 -0.707830 -0.474722  0.763908 -0.869485 -0.349489 -0.293731   \n",
       "\n",
       "         x8        x9       x10  ...       x32       x33       x34       x35  \\\n",
       "0  0.156825 -0.876694  0.620652  ...  0.438694 -0.019702 -0.600950 -0.877849   \n",
       "1  0.502329  0.981269 -0.620777  ... -0.281196 -0.885851  0.416578 -0.820090   \n",
       "2 -0.677216  0.611557  0.917289  ... -0.104604  0.267912 -0.165641  0.509045   \n",
       "3  0.881861  0.521265  0.413126  ...  0.085894 -0.455087  0.662139 -0.846376   \n",
       "4  0.929615  0.450917 -0.781646  ... -0.014800  0.404468 -0.225323  0.733259   \n",
       "\n",
       "        x36       x37       x38       x39       x40          y  \n",
       "0  0.569724 -0.932564  0.290253 -0.763400  0.935585 -63.087329  \n",
       "1  0.539046 -0.602434 -0.507292 -0.717584  0.678476 -61.257623  \n",
       "2  0.703101  0.427531  0.142499 -0.754593 -0.924559 -58.916994  \n",
       "3  0.979447 -0.876542 -0.576039  0.242609 -0.890070 -56.749217  \n",
       "4  0.470233  0.830019  0.199850 -0.435511 -0.277896 -56.247766  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x32</th>\n",
       "      <th>x33</th>\n",
       "      <th>x34</th>\n",
       "      <th>x35</th>\n",
       "      <th>x36</th>\n",
       "      <th>x37</th>\n",
       "      <th>x38</th>\n",
       "      <th>x39</th>\n",
       "      <th>x40</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.015239</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.012350</td>\n",
       "      <td>-0.021853</td>\n",
       "      <td>-0.005699</td>\n",
       "      <td>0.030694</td>\n",
       "      <td>0.016195</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>-0.041889</td>\n",
       "      <td>-0.034891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>-0.014550</td>\n",
       "      <td>-0.002691</td>\n",
       "      <td>-0.026971</td>\n",
       "      <td>0.032099</td>\n",
       "      <td>-0.020810</td>\n",
       "      <td>-0.001286</td>\n",
       "      <td>0.007160</td>\n",
       "      <td>0.033056</td>\n",
       "      <td>1.032572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.579666</td>\n",
       "      <td>0.575857</td>\n",
       "      <td>0.578428</td>\n",
       "      <td>0.562803</td>\n",
       "      <td>0.567329</td>\n",
       "      <td>0.585413</td>\n",
       "      <td>0.575098</td>\n",
       "      <td>0.580830</td>\n",
       "      <td>0.581383</td>\n",
       "      <td>0.581479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579899</td>\n",
       "      <td>0.581358</td>\n",
       "      <td>0.571397</td>\n",
       "      <td>0.577279</td>\n",
       "      <td>0.570002</td>\n",
       "      <td>0.572490</td>\n",
       "      <td>0.586571</td>\n",
       "      <td>0.567041</td>\n",
       "      <td>0.556633</td>\n",
       "      <td>21.459568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.999639</td>\n",
       "      <td>-0.999907</td>\n",
       "      <td>-0.999578</td>\n",
       "      <td>-0.997341</td>\n",
       "      <td>-0.997749</td>\n",
       "      <td>-0.997432</td>\n",
       "      <td>-0.999388</td>\n",
       "      <td>-0.999746</td>\n",
       "      <td>-0.999455</td>\n",
       "      <td>-0.999715</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999575</td>\n",
       "      <td>-0.997640</td>\n",
       "      <td>-0.998386</td>\n",
       "      <td>-0.999950</td>\n",
       "      <td>-0.999410</td>\n",
       "      <td>-0.993079</td>\n",
       "      <td>-0.999102</td>\n",
       "      <td>-0.998609</td>\n",
       "      <td>-0.999760</td>\n",
       "      <td>-63.087329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.515578</td>\n",
       "      <td>-0.512347</td>\n",
       "      <td>-0.488257</td>\n",
       "      <td>-0.492955</td>\n",
       "      <td>-0.479871</td>\n",
       "      <td>-0.476162</td>\n",
       "      <td>-0.473717</td>\n",
       "      <td>-0.495279</td>\n",
       "      <td>-0.548818</td>\n",
       "      <td>-0.552028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.503873</td>\n",
       "      <td>-0.533043</td>\n",
       "      <td>-0.499724</td>\n",
       "      <td>-0.524747</td>\n",
       "      <td>-0.454333</td>\n",
       "      <td>-0.502945</td>\n",
       "      <td>-0.533903</td>\n",
       "      <td>-0.480978</td>\n",
       "      <td>-0.444436</td>\n",
       "      <td>-14.254513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.006295</td>\n",
       "      <td>0.013728</td>\n",
       "      <td>0.040301</td>\n",
       "      <td>-0.022134</td>\n",
       "      <td>-0.007999</td>\n",
       "      <td>0.054535</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>-0.024722</td>\n",
       "      <td>-0.062433</td>\n",
       "      <td>-0.039670</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008584</td>\n",
       "      <td>-0.022289</td>\n",
       "      <td>0.010255</td>\n",
       "      <td>-0.034473</td>\n",
       "      <td>0.023374</td>\n",
       "      <td>-0.059977</td>\n",
       "      <td>-0.009772</td>\n",
       "      <td>0.012218</td>\n",
       "      <td>0.041591</td>\n",
       "      <td>1.431842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.463495</td>\n",
       "      <td>0.497554</td>\n",
       "      <td>0.510812</td>\n",
       "      <td>0.426922</td>\n",
       "      <td>0.473263</td>\n",
       "      <td>0.558355</td>\n",
       "      <td>0.496908</td>\n",
       "      <td>0.510023</td>\n",
       "      <td>0.473278</td>\n",
       "      <td>0.472946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527520</td>\n",
       "      <td>0.471522</td>\n",
       "      <td>0.498615</td>\n",
       "      <td>0.466222</td>\n",
       "      <td>0.532116</td>\n",
       "      <td>0.486100</td>\n",
       "      <td>0.513547</td>\n",
       "      <td>0.480789</td>\n",
       "      <td>0.521005</td>\n",
       "      <td>16.936206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.999663</td>\n",
       "      <td>0.999737</td>\n",
       "      <td>0.992472</td>\n",
       "      <td>0.999806</td>\n",
       "      <td>0.998955</td>\n",
       "      <td>0.996246</td>\n",
       "      <td>0.999716</td>\n",
       "      <td>0.998040</td>\n",
       "      <td>0.999384</td>\n",
       "      <td>0.999262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998586</td>\n",
       "      <td>0.997789</td>\n",
       "      <td>0.999548</td>\n",
       "      <td>0.997747</td>\n",
       "      <td>0.997131</td>\n",
       "      <td>0.998377</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.999434</td>\n",
       "      <td>0.994621</td>\n",
       "      <td>62.620466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                x1           x2           x3           x4           x5  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean     -0.015239     0.002455     0.012350    -0.021853    -0.005699   \n",
       "std       0.579666     0.575857     0.578428     0.562803     0.567329   \n",
       "min      -0.999639    -0.999907    -0.999578    -0.997341    -0.997749   \n",
       "25%      -0.515578    -0.512347    -0.488257    -0.492955    -0.479871   \n",
       "50%      -0.006295     0.013728     0.040301    -0.022134    -0.007999   \n",
       "75%       0.463495     0.497554     0.510812     0.426922     0.473263   \n",
       "max       0.999663     0.999737     0.992472     0.999806     0.998955   \n",
       "\n",
       "                x6           x7           x8           x9          x10  ...  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
       "mean      0.030694     0.016195     0.002731    -0.041889    -0.034891  ...   \n",
       "std       0.585413     0.575098     0.580830     0.581383     0.581479  ...   \n",
       "min      -0.997432    -0.999388    -0.999746    -0.999455    -0.999715  ...   \n",
       "25%      -0.476162    -0.473717    -0.495279    -0.548818    -0.552028  ...   \n",
       "50%       0.054535     0.006247    -0.024722    -0.062433    -0.039670  ...   \n",
       "75%       0.558355     0.496908     0.510023     0.473278     0.472946  ...   \n",
       "max       0.996246     0.999716     0.998040     0.999384     0.999262  ...   \n",
       "\n",
       "               x32          x33          x34          x35          x36  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.000630    -0.014550    -0.002691    -0.026971     0.032099   \n",
       "std       0.579899     0.581358     0.571397     0.577279     0.570002   \n",
       "min      -0.999575    -0.997640    -0.998386    -0.999950    -0.999410   \n",
       "25%      -0.503873    -0.533043    -0.499724    -0.524747    -0.454333   \n",
       "50%      -0.008584    -0.022289     0.010255    -0.034473     0.023374   \n",
       "75%       0.527520     0.471522     0.498615     0.466222     0.532116   \n",
       "max       0.998586     0.997789     0.999548     0.997747     0.997131   \n",
       "\n",
       "               x37          x38          x39          x40            y  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean     -0.020810    -0.001286     0.007160     0.033056     1.032572  \n",
       "std       0.572490     0.586571     0.567041     0.556633    21.459568  \n",
       "min      -0.993079    -0.999102    -0.998609    -0.999760   -63.087329  \n",
       "25%      -0.502945    -0.533903    -0.480978    -0.444436   -14.254513  \n",
       "50%      -0.059977    -0.009772     0.012218     0.041591     1.431842  \n",
       "75%       0.486100     0.513547     0.480789     0.521005    16.936206  \n",
       "max       0.998377     0.999819     0.999434     0.994621    62.620466  \n",
       "\n",
       "[8 rows x 41 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part (a): To see how the populatiton size affect the model's performance, randomly take samples of size 100, 150, 200, ...., 950, 1000 (i.e., incrementing sample sizes by 50) from the given dataset, apply train-test split (%70 train-%30 test), fit a multiple linear regression model, evaluate the mean square performance of the model on the train and test sets and plot results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly take samples of size 50 to 1000=>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_error</th>\n",
       "      <th>test_error</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>9.809585e-28</td>\n",
       "      <td>74.544390</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2.695500e+00</td>\n",
       "      <td>14.766978</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>3.822463e+00</td>\n",
       "      <td>9.379023</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>5.064987e+00</td>\n",
       "      <td>8.260727</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>4.839934e+00</td>\n",
       "      <td>10.628142</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>5.650585e+00</td>\n",
       "      <td>10.181128</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>5.983976e+00</td>\n",
       "      <td>8.404781</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>5.563155e+00</td>\n",
       "      <td>9.818694</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>5.897382e+00</td>\n",
       "      <td>7.667577</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>6.101554e+00</td>\n",
       "      <td>8.377569</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>6.209688e+00</td>\n",
       "      <td>8.266643</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>6.419165e+00</td>\n",
       "      <td>7.538810</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>6.247288e+00</td>\n",
       "      <td>7.315945</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>6.487223e+00</td>\n",
       "      <td>7.255918</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>6.449012e+00</td>\n",
       "      <td>7.926206</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>6.738211e+00</td>\n",
       "      <td>6.846194</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>6.825613e+00</td>\n",
       "      <td>7.273110</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>6.720363e+00</td>\n",
       "      <td>7.287113</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>6.771469e+00</td>\n",
       "      <td>6.927094</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>6.647318e+00</td>\n",
       "      <td>7.041621</td>\n",
       "      <td>LinearRegression()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train_error  test_error               model\n",
       "size                                              \n",
       "50    9.809585e-28   74.544390  LinearRegression()\n",
       "100   2.695500e+00   14.766978  LinearRegression()\n",
       "150   3.822463e+00    9.379023  LinearRegression()\n",
       "200   5.064987e+00    8.260727  LinearRegression()\n",
       "250   4.839934e+00   10.628142  LinearRegression()\n",
       "300   5.650585e+00   10.181128  LinearRegression()\n",
       "350   5.983976e+00    8.404781  LinearRegression()\n",
       "400   5.563155e+00    9.818694  LinearRegression()\n",
       "450   5.897382e+00    7.667577  LinearRegression()\n",
       "500   6.101554e+00    8.377569  LinearRegression()\n",
       "550   6.209688e+00    8.266643  LinearRegression()\n",
       "600   6.419165e+00    7.538810  LinearRegression()\n",
       "650   6.247288e+00    7.315945  LinearRegression()\n",
       "700   6.487223e+00    7.255918  LinearRegression()\n",
       "750   6.449012e+00    7.926206  LinearRegression()\n",
       "800   6.738211e+00    6.846194  LinearRegression()\n",
       "850   6.825613e+00    7.273110  LinearRegression()\n",
       "900   6.720363e+00    7.287113  LinearRegression()\n",
       "950   6.771469e+00    6.927094  LinearRegression()\n",
       "1000  6.647318e+00    7.041621  LinearRegression()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  create dataframe whÄ±ch have information about errors, models and size of our population.\n",
    "results = pd.DataFrame(columns =['train_error', 'test_error', 'model', 'size']).set_index('size')  \n",
    "\n",
    "\n",
    "print ('Randomly take samples of size 50 to 1000=>')\n",
    "for size in range(50,1001, 50):   #ramdomly take samples 50 to 1000 sizes\n",
    "    sample = df.sample(n=size, random_state=1)\n",
    "    X = sample.iloc[:, :-1].values   # take featues\n",
    "    y = sample.iloc[:, -1].values    # take target variable\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=0)  # split data into test and train\n",
    "    \n",
    "    regressor = LinearRegression()    # using regression model\n",
    "    regressor.fit(X_train, y_train)   # fit model\n",
    "    \n",
    "    y_pred = regressor.predict(X_test)  #prediction for test data\n",
    "    \n",
    "    y_pred_train = regressor.predict(X_train)   #prediction for train data\n",
    "    \n",
    "    test_error = metrics.mean_squared_error(y_test, y_pred)    #mean square error for test data\n",
    "    train_error = metrics.mean_squared_error(y_train, y_pred_train)   # mean square error for train data\n",
    "    \n",
    "    results.loc[size] = train_error, test_error, regressor   # adding our infos into results dataframe.\n",
    "    \n",
    "results\n",
    "\n",
    "    #print('Mean Squared Error:',  metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'part (a) train-testing errors')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vizulation our results:\n",
    "results.plot()\n",
    "plt.grid()\n",
    "plt.title(\"part (a) train-testing errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with plotting training and testing errors we can see model size and their changing. we can see that in first step until 200 size population, test error decreasing dramaticly but train error increases a little. and they go like stable after 200 size. we can say that sÄ±ze of 200 is enough for our dataset.\n",
    "With more pupulation, test_error is decreasing. The learning curve working in your advantage, ie. you can train better models given more data for testing thus test_error is decreasing. Moreover, train_error increasing because it is harder for the model (with a fixed complexity) to overfit to a bigger training set. After one point, performance stay same. In some situation, increasing the number of population does not means to improve performance of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part(b): Repeat part (a) after applying polynomial expansion with degree=2 and set to the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create dataframe whÄ±ch have information about errors, models and size of our population.\n",
    "results1 = pd.DataFrame(columns =['train_error1', 'test_error1', 'model1', 'size1']).set_index('size1')\n",
    "\n",
    "for size in range(50,1000, 50):\n",
    "    sample = df.sample(n=size, random_state=1)\n",
    "    X = sample.iloc[:, :-1].values\n",
    "    y = sample.iloc[:, -1].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=0)\n",
    "    \n",
    "    regressor = make_pipeline(PolynomialFeatures(degree=2), LinearRegression(fit_intercept = False))\n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = regressor.predict(X_test)\n",
    "    \n",
    "    y_pred_train = regressor.predict(X_train)\n",
    "    \n",
    "    test_error = metrics.mean_squared_error(y_test, y_pred)\n",
    "    train_error = metrics.mean_squared_error(y_train, y_pred_train)\n",
    "    \n",
    "    results1.loc[size] = train_error, test_error, regressor\n",
    "    \n",
    "    \n",
    "    #print('Mean Squared Error:',  metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_error1</th>\n",
       "      <th>test_error1</th>\n",
       "      <th>model1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2.062697e-26</td>\n",
       "      <td>182.518866</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>5.197889e-27</td>\n",
       "      <td>259.151633</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2.152287e-27</td>\n",
       "      <td>233.031456</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>3.450992e-27</td>\n",
       "      <td>214.910863</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>3.783241e-27</td>\n",
       "      <td>216.374402</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>4.619606e-27</td>\n",
       "      <td>167.671730</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>2.630606e-27</td>\n",
       "      <td>161.324026</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>6.532476e-27</td>\n",
       "      <td>129.533477</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>8.949015e-27</td>\n",
       "      <td>126.963729</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>6.877607e-27</td>\n",
       "      <td>98.867871</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>3.474266e-27</td>\n",
       "      <td>87.578676</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>1.004542e-26</td>\n",
       "      <td>80.475561</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>6.882777e-27</td>\n",
       "      <td>62.803964</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>5.560329e-27</td>\n",
       "      <td>54.588905</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>7.217580e-27</td>\n",
       "      <td>44.774643</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>3.260831e-27</td>\n",
       "      <td>37.055881</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>4.256739e-27</td>\n",
       "      <td>36.326958</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>3.947197e-27</td>\n",
       "      <td>21.837292</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>1.019028e-26</td>\n",
       "      <td>19.776313</td>\n",
       "      <td>(PolynomialFeatures(), LinearRegression(fit_in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train_error1  test_error1  \\\n",
       "size1                              \n",
       "50     2.062697e-26   182.518866   \n",
       "100    5.197889e-27   259.151633   \n",
       "150    2.152287e-27   233.031456   \n",
       "200    3.450992e-27   214.910863   \n",
       "250    3.783241e-27   216.374402   \n",
       "300    4.619606e-27   167.671730   \n",
       "350    2.630606e-27   161.324026   \n",
       "400    6.532476e-27   129.533477   \n",
       "450    8.949015e-27   126.963729   \n",
       "500    6.877607e-27    98.867871   \n",
       "550    3.474266e-27    87.578676   \n",
       "600    1.004542e-26    80.475561   \n",
       "650    6.882777e-27    62.803964   \n",
       "700    5.560329e-27    54.588905   \n",
       "750    7.217580e-27    44.774643   \n",
       "800    3.260831e-27    37.055881   \n",
       "850    4.256739e-27    36.326958   \n",
       "900    3.947197e-27    21.837292   \n",
       "950    1.019028e-26    19.776313   \n",
       "\n",
       "                                                  model1  \n",
       "size1                                                     \n",
       "50     (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "100    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "150    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "200    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "250    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "300    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "350    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "400    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "450    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "500    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "550    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "600    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "650    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "700    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "750    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "800    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "850    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "900    (PolynomialFeatures(), LinearRegression(fit_in...  \n",
       "950    (PolynomialFeatures(), LinearRegression(fit_in...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'part (b) train-testing errors')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results1.plot()\n",
    "plt.grid()\n",
    "plt.title(\"part (b) train-testing errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the train error stays same so there can be overfitting. As sample size is incresing, the overfitting issue may be overcomed. The learning curve working in your advantage, ie. you can train better models given more data for testing thus test error is decreasing. We can change degrre of polynomial expentionÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part(c): Use Lasso regularization and try to fit the best possible model to the given dataset. Use (%70 train - %30 test) splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error for training 2.6045214337006177\n",
      "R2 for training 0.9852870968560583\n",
      "Mean Square Error for testing 2.610076780873246\n",
      "R2 for testing 0.9852783880282705\n"
     ]
    }
   ],
   "source": [
    "X = sample.iloc[:, :-1].values\n",
    "y = sample.iloc[:, -1].values\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=0)\n",
    "\n",
    "model_lasso = Lasso(alpha=0.01)\n",
    "\n",
    "model_lasso.fit(X_train, y_train) \n",
    "\n",
    "pred_train_lasso= model_lasso.predict(X_train)\n",
    "\n",
    "print(\"Mean Square Error for training\",np.sqrt(mean_squared_error(y_train,pred_train_lasso)))\n",
    "print(\"R2 for training\",r2_score(y_train, pred_train_lasso))\n",
    "\n",
    "pred_test_lasso= model_lasso.predict(X_test)\n",
    "\n",
    "print(\"Mean Square Error for testing\",np.sqrt(mean_squared_error(y_test,pred_test_lasso))) \n",
    "print(\"R2 for testing\",r2_score(y_test, pred_test_lasso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that in lasso model, we havre alpha = 0.01. if we increse the alpha our coefficents goes to the zero but our erros (mse,r2) will same similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
